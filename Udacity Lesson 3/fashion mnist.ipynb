{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Fashion Minst数据集进行服装分类\n",
    "在此次样例中，将建立神经网络对服装图片进行分类。一共十类，分别为：\n",
    "> class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "### 输入\n",
    "28 * 28黑白图片\n",
    "### 输出\n",
    "数字0-9表示类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "# Import TensorFlow and TensorFlow Datasets\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import math\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据\n",
    "由于使用tensorflow_datasets加载Fashion Mnist数据集时，会因为网络原因导致无法加载，因此将数据下载到本地。\n",
    "\n",
    "数据下载地址：<a href='https://github.com/zalandoresearch/fashion-mnist/tree/master/data/fashion'>https://github.com/zalandoresearch/fashion-mnist/tree/master/data/fashion</a>\n",
    "\n",
    "使用gzip进行文件读取操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    base = \"./fashion mnist data/\"\n",
    "    files = [\n",
    "        'train-labels-idx1-ubyte.gz', 'train-images-idx3-ubyte.gz',\n",
    "        't10k-labels-idx1-ubyte.gz', 't10k-images-idx3-ubyte.gz'\n",
    "    ]\n",
    "\n",
    "\n",
    "    with gzip.open(base + files[0], 'rb') as lbpath:\n",
    "        y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(base + files[1], 'rb') as imgpath:\n",
    "        x_train = np.frombuffer(\n",
    "            imgpath.read(), np.uint8, offset=16).reshape(len(y_train), 28, 28)\n",
    "\n",
    "    with gzip.open(base + files[2], 'rb') as lbpath:\n",
    "        y_test = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(base + files[3], 'rb') as imgpath:\n",
    "        x_test = np.frombuffer(\n",
    "            imgpath.read(), np.uint8, offset=16).reshape(len(y_test), 28, 28)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-shirt/top\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "(train_images, train_labels), (test_images, test_labels) = load_data()\n",
    "\n",
    "# 定义不同类别的名称\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# 获取训练集以及测试集中样本的数量\n",
    "TRAINING_SAMPLE_NUM = train_images.shape[0]\n",
    "TEST_SAMPLE_NUM = test_images.shape[0]\n",
    "print(TRAINING_SAMPLE_NUM, TEST_SAMPLE_NUM)\n",
    "\n",
    "# 取出一张图片进行展示\n",
    "plt.figure()\n",
    "plt.imshow(train_images[1], cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "print(class_names[train_labels[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据变为``tf.data.Dataset``的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将图片数据的像素值从0-255变为0-1的浮点数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(images, labels):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images /= 255\n",
    "    return images, labels\n",
    "\n",
    "train_dataset = train_dataset.map(normalize)\n",
    "test_dataset = test_dataset.map(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型\n",
    "* **input layer** `tf.keras.layers.Flatten` — This layer transforms the images from a 2d-array of 28 $\\times$ 28 pixels, to a 1d-array of 784 pixels (28\\*28). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn, as it only reformats the data.\n",
    "* **hidden layer** `tf.keras.layers.Dense`— A densely connected layer of 128 neurons. Each neuron (or node) takes input from all 784 nodes in the previous layer, weighting that input according to hidden parameters which will be learned during training, and outputs a single value to the next layer.\n",
    "* **output layer** `tf.keras.layers.Dense` — A 10-node *softmax* layer, with each node representing a class of clothing. As in the previous layer, each node takes input from the 128 nodes in the layer before it. Each node weights the input according to learned parameters, and then outputs a value in the range `[0, 1]`, representing the probability that the image belongs to that class. The sum of all 10 node values is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编译模型\n",
    "* ``loss``这里使用了``tf.keras.losses.sparse_categorical_crossentropy``作为损失函数。``tf.keras.losses.sparse_categorical_crossentropy``与``tf.keras.losses.categorical_crossentropy``的区别为：\n",
    "    * ``tf.keras.losses.categorical_crossentropy``的``y_true``为``one-hot``编码形式。TensorFlow官方文档中对``tf.keras.losses.CategoricalCrossentropy``类的描述为\n",
    "    > Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a ``one_hot`` representation. If you want to provide labels as integers, please use ``SparseCategoricalCrossentropy`` loss. There should be ``# classes`` floating point values per feature.\n",
    "    * ``tf.keras.losses.sparse_categorical_crossentropy``的``y_true``为数字形式。TensorFlow官方文档中对``tf.keras.losses.SparseCategoricalCrossentropy``类的描述为\n",
    "    > Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using ``one-hot`` representation, please use ``CategoricalCrossentropy`` loss. There should be ``# classes`` floating point values per feature for ``y_pred`` and a single floating point value per feature for ``y_true``.\n",
    "* ``optimizer``这里依然选择使用效果最好的``Adam``优化方法\n",
    "* ``metrics``这里传入的参数为``[accuracy]``，会自动选择合适的方法进行效果评估。适合此样例的评估方法为``tf.keras.metrics.sparse_categorical_accuracy``。``tf.keras.metrics.Accuracy()``、``tf.keras.metrics.categorical_accuracy``、``tf.keras.metrics.sparse_categorical_accuracy``三者的区别为：\n",
    "    * ``tf.keras.metrics.Accuracy()``的``y_true``和``y_pred``均是数字，如：``y_true`` is [1, 2, 3, 4] and ``y_pred`` is [0, 2, 3, 4]\n",
    "    * ``tf.keras.metrics.categorical_accuracy``的``y_true``是``one-hot``，而``y_pred``是维度与类别数相同的一维数组，如：``y_true`` is [[0, 0, 1], [0, 1, 0]] and ``y_pred`` is [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n",
    "    * ``tf.keras.metrics.sparse_categorical_accuracy``的``y_true``是数字，而``y_pred``是维度与类别数相同的一维数组，如：``y_true`` is [[2], [1]] and ``y_pred`` is [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntf.keras.metrics.Accuracy()\\ntf.keras.metrics.categorical_accuracy\\ntf.keras.metrics.sparse_categorical_accuracy\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "             optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "'''\n",
    "tf.keras.metrics.Accuracy()\n",
    "tf.keras.metrics.categorical_accuracy\n",
    "tf.keras.metrics.sparse_categorical_accuracy\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.repeat().shuffle(TRAINING_SAMPLE_NUM).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 15:35:24.799915  5528 deprecation.py:323] From C:\\Users\\arsener\\PycharmProjects\\tensorflow2.0beta\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.5163 - accuracy: 0.8176\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4430 - accuracy: 0.8396\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4109 - accuracy: 0.8509\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4040 - accuracy: 0.8550\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3899 - accuracy: 0.8592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27f3d816278>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=5, steps_per_epoch=math.ceil(TRAINING_SAMPLE_NUM / BATCH_SIZE), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4647 - accuracy: 0.8405\n",
      "Loss and accuracy on test dataset: 0.4647, 0.8405\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset, steps=math.ceil(TEST_SAMPLE_NUM / BATCH_SIZE))\n",
    "print('Loss and accuracy on test dataset: {0:.4f}, {1:.4f}'.format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.0beta",
   "language": "python",
   "name": "tensorflow2.0beta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
